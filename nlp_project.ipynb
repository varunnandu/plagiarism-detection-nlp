{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from sklearn.cluster import KMeans\n",
    "from numbers import Number\n",
    "from pandas import DataFrame\n",
    "import sys, codecs, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class autovivify_list(dict):\n",
    "  '''A pickleable version of collections.defaultdict'''\n",
    "  def __missing__(self, key):\n",
    "    '''Given a missing key, set initial value to an empty list'''\n",
    "    value = self[key] = []\n",
    "    return value\n",
    "\n",
    "  def __add__(self, x):\n",
    "    '''Override addition for numeric types when self is empty'''\n",
    "    if not self and isinstance(x, Number):\n",
    "      return x\n",
    "    raise ValueError\n",
    "\n",
    "  def __sub__(self, x):\n",
    "    '''Also provide subtraction method'''\n",
    "    if not self and isinstance(x, Number):\n",
    "      return -1 * x\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file, n_words):\n",
    "  '''Return the vectors and labels for the first n_words in vector file'''\n",
    "  numpy_arrays = []\n",
    "  labels_array = []\n",
    "  with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "    for c, r in enumerate(f):\n",
    "      sr = r.split()\n",
    "      labels_array.append(sr[0])\n",
    "      numpy_arrays.append( numpy.array([float(i) for i in sr[1:]]) )\n",
    "\n",
    "      if c == n_words:\n",
    "        return numpy.array( numpy_arrays ), labels_array\n",
    "\n",
    "  return numpy.array( numpy_arrays ), labels_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_word_clusters(labels_array, cluster_labels):\n",
    "  '''Return the set of words in each cluster'''\n",
    "  cluster_to_words = autovivify_list()\n",
    "  for c, i in enumerate(cluster_labels):\n",
    "    cluster_to_words[ i ].append( labels_array[c] )\n",
    "  return cluster_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=252, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector_file = '/Users/varunnandu/glove/vectors.txt' # Vector file input (e.g. glove.6B.300d.txt)\n",
    "n_words = int(1262) # Number of words to analyze \n",
    "reduction_factor = float(0.2) # Amount of dimension reduction {0,1}\n",
    "n_clusters = int( n_words * reduction_factor ) # Number of clusters to make\n",
    "df, labels_array = build_word_vector_matrix(input_vector_file, n_words)\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=n_clusters, n_init=10)\n",
    "kmeans_model.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels  = kmeans_model.labels_\n",
    "cluster_inertia   = kmeans_model.inertia_\n",
    "cluster_to_words  = find_word_clusters(labels_array, cluster_labels)\n",
    "\n",
    "clusters = []\n",
    "for c in cluster_to_words:\n",
    "    clusters.append(cluster_to_words[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      ['the', 'of', 'is', 'to', 'and', '05', 'in', 'that', 'as', 'it', 'by', 'are', 'for', 'this', 'or', 'probability', 'used', 'which', 'an', 'vector', 'pagerank', 'term', 'on', 'from', 'classes', 'he', 'has', 'with', 'have', 'page', 'one', 'inheritance', 'optimal', 'each', 'probabilities', 'google', 'also', 'class', 'number', 'called', 'links', 'given', 'we', 'example', 'between', 'new', 'where', 'if', 'using', 'any', 'many', 'at', 'two', 'known', 'objects', 'been', 'different', 'no', 'there', 'word', 'represented', 'same', 'so', 'inherit', 'they', 'because', 'program', 'derived', 'often', 'sometimes', 'based', 'relates', 'after', 'base', 'finding', 'overall', 'occurs', 'algebraic']\n",
      "8      ['be', 'can']\n",
      "17      ['programming', 'dynamic']\n",
      "27      ['h0k', 'ubx', 'hhk', 'hij', 'hijses']\n",
      "29      ['all', 'common']\n",
      "31      ['theorem', 'bayes']\n",
      "35      ['document', 'query', 'original']\n",
      "37      ['06', '000750848', '00152028', '41461e']\n",
      "39      ['problems', 'method', 'solving']\n",
      "40      ['documents', 'vectors']\n",
      "43      ['information', 'retrieval']\n",
      "45      ['space', 'model']\n",
      "49      ['subproblems', 'solutions']\n",
      "51      ['terms', 'words']\n",
      "56      ['not', 'does']\n",
      "57      ['these', 'then', 'could', 'would', 'calculated']\n",
      "58      ['problem', 'solution', 'find', 'solve', 'need', 'you']\n",
      "59      ['conditional', 'marginal']\n",
      "61      ['other', 'pages', 'web']\n",
      "63      ['more', 'methods']\n",
      "65      ['may', 'when', 'certain', 'observed', 'applied', 'symptoms']\n",
      "66      ['value', 'its', 'importance', 'within']\n",
      "71      ['events', 'random']\n",
      "72      ['object', 'oriented']\n",
      "73      ['about', 'specific', 'needs', 'stored']\n",
      "74      ['algorithm', 'link', 'analysis']\n",
      "75      ['sub', 'into']\n",
      "77      ['means', 'sense']\n",
      "81      ['search', 'than', 'bellman', 'typically', 'having', '1940s', 'engines', 'richard', 'comparing', 'normally', 'specialised']\n",
      "82      ['computer', 'science', 'mathematics']\n",
      "84      ['was', 'first']\n",
      "85      ['8i', '8q', 'o8', '8h', '98', 'h8', '8d', '8f', '8u', '8g', 'y8', 'q8', 'w8', 'z8', '7w', '48', '38', '7d', '8s', 'b9', '7y', '7u', 'relationship', '7t', 'j7', '82', 'ja', 'n9', '6a', 'jj', 'second', 'vote', 'assumptions', 'acts', 'cx', 'df', 'fu', 'jm', 'qd']\n",
      "88      ['such', 'well']\n",
      "89      ['8w', 'time', '8j', 'b8', '8l', 'd8', 'r8', 't8', '8n', 'x8', 'k8', '7o', '7p', '87', '79', '84', '7v', '97', 'j8', '28', '78', '7a', '7q', '8r', '7f', '9s', 'v7', '18', '7c', '7h', '7k', 'data', 'f7', 'g9', '7b', '7i', '86', 'collection', 'every', 'i7', 'main', '19', '7e', '7s', '7x', '83', '_8', '77', '7m', 'c7', 'generally', 'see', 'zs', '6b', '6v', '72', '7_', '7l', 'b7', 'factor', 'following', 'k7', 'made', '6e', '73', '76', '8_', '96', 'ae', 'dx', 'fs', 'idea', 'lg', 'md', 'memoization', 'rd', 'rj', 'zn', '37', '4e', '5x', '67', '6i', '6l', '70', '93', 'ac', 'accurate', 'bd', 'd7', 'fi', 'formed', 'g6', 'ii', 'il', 'inherited', 'iv', 'j6', 'lt', 'un', 'vl', 'wz', 'xl', 'xt']\n",
      "90      ['8x', '8k', '9m', 'g8', '8a', '9h', 'h9', 'u8', '9i', 'o9', '000226815', '9q', 'c9', 'since', 'x9', 'q9', '9d', '9o', '95', 'describe', '29', '49', '85', 'dp', 'y9', '7j', 'entities', 'ir', '9_', 'i9', 'k9', 'm9', 'q7', 'a1', 'bayesians', 'pr', 's9', 'ff', 'processing', 'uh', 'xp', 'bw', 'equation', 'hg', 'mechanism', 'ny', 'postgraduate', 'transport', 'whole', '1g', 'animals', 'ch', 'content', 'eh', 'engineering', 'fd', 'features', 'four', 'further', 'get', 'just', 'jz', 'nodes', 'patented', 'pieces', 'public', 'qs', 'qz', 'recursively', 'superclasses', 'without', 'yr']\n",
      "91      ['code', 'their', 'frequencies', 'subsets', 'occurrence']\n",
      "92      ['up', 'superclass', 'only', 'subclass', 'out', 'found']\n",
      "93      ['values', 'similarity', 'poor']\n",
      "94      ['student', 'distribution', 'topic', 'girl', 'popularity', 'possible', 'who', 'discuss', 'givenis', 'happened', 'previous', 'right', 'theoretical', 'true']\n",
      "95      ['way', 'form']\n",
      "96      ['will', 'important', 'results', 'uses', 'high', 'internet', 'name', 'rank', 'sites', 'determine']\n",
      "97      ['8e', '8v', '8z', 'l8', 'c8', 'p8', '8m', '8p', 's8', '9t', 'i8', '8o', '8t', 'a8', 'f8', '89', '9y', 'l7', 'hence', 'calculating', 'uncertainty', 'angles', 'chance', 'counts', 'evidence', 'happening', 'systemdocument', 'type']\n",
      "98      ['being', 'models', 'poorly', 'queries', 'systems', 'expressed', 'populations', 'instances', 'long', 'proportions']\n",
      "102      ['fruit', 'apple', 'acceptable', 'abstraction']\n",
      "105      ['9j', '9x', '9n', '9e', '9p', '9f', '9l', '9v', '99', '9u', 'j9', '9c', 'orange', 'etc', 'takes', 'down', 'top', 'disagree', 'wearing', 'below', 'wear', 'described', 'weightingthe']\n",
      "106      ['compute', 'posterior', 'calculate']\n",
      "107      ['however', 'most', 'use', 'another', 'instead', 'therefore', 'usually', 'allows', 'system', 'useful', 'what', 'formula', 'considered']\n",
      "109      ['8b', '88', '9g', 'm8', 'n8', 'v8', '9b', 'u9', 'r7', 'v9', 'w7', '7r', 'p9', '58', '68', '7z', '81', '8c', 'a9', '9r', 'e8', 'e9', 'l9', 'y7', '7g', '9a', 'd9', 'f9', '09', '6m', 'm7', 'r9', '08', '10', '69', '90', 'a7', 'available', 'g7', 'h7', 'key', 'u7', 'z7', 'z9', '59', '7n', '91', 'e7', 'include', 'n7', 'om', 'w9', 'whilst', 'ht', 'kw', 'popular', 'qw', 'ra', 'research', 'ro', 'ss', 'very', 'yz', 'z6', '17', '6r', '74', '_9', 'bm', 'd6', 'da', 'dc', 'es', 'fj', 'fk', 'functions', 'fv', 'gb', 'gr', 'h1', 'h6', 'hc', 'io', 'kf', 'km', 'ku', 'kx', 'language', 'make', 'measure', 'mt', 'og', 'pp', 'qf', 'qj', 'ri', 'surfer', 'together', 'ui', 'uq', 'wa', 'ya', 'zp', '<unk>']\n",
      "111      ['properties', 'substructure', 'overlapping', 'process', 'exhibit']\n",
      "112      ['8y', '9w', '9k', '80', '9z', '2099e', 't9', 'naive', 'x7', '92', 'apples', 'numerical', '39', '75', 'de', 'must', 'rq', 'circle', 'f5', 'qx', 'ba', 'context', 'conversely', 'numeric', 'our', 'pre', 'precisely', 'smaller', 'yn', '60', 'co', 'entity', 'gi', 'nq', 'reach', 'shape', 'st', 'storing']\n",
      "113      ['but', 'some', 'computed', 'say', 'do', 'even', 'relation', 'save', 'try']\n",
      "114      ['theory', 'prior']\n",
      "117      ['both', 'attributes', 'beliefs', 'subclasses', 'degrees', 'unique', 'types', 'behavior', 'behaviours']\n",
      "118      ['how', 'linked']\n",
      "120      ['set', 'element', 'identifiers', 'hyperlinked', 'index', 'container', 'fleshy', 'seed']\n",
      "121      ['computing', 'f2', 'f3', 'f4', 'product', 'train', 'large', 'involves', 'reciprocal', 'small', 'car', 'f1', 'mango', 'quotations']\n",
      "122      ['best', 'decisions']\n",
      "124      ['generalization', 'hierarchy', 'represent', 'relationships', 'schemes']\n",
      "125      ['non', 'zero', 'several', 'frequency']\n",
      "126      ['order', 'result', 'vocabulary', 'appear', 'lost', 'occurring']\n",
      "127      ['bayesian', 'interpretations', 'frequentist', 'statistics']\n",
      "128      ['existing', 'reuse', 'help']\n",
      "130      ['similar', 'share', 'lot', 'interfaces', 'modules', 'sufficiently']\n",
      "132      ['ways', 'describes']\n",
      "133      ['already', 'defined']\n",
      "134      ['approach', 'extend', 'larger', 'needed', 'patient', 're', 'intended', 'cannot', 'chosen', 'consider', 'create', 'due', 'group', 'solves', 'access', 'add', 'again', 'manipulation', 'said']\n",
      "136      ['optimization', 'mathematical', 'languages', 'synonym']\n",
      "137      ['particular', 'connection']\n",
      "138      ['solved', 'variables', 'them', 'divided', 'themselves', 'updated']\n",
      "139      ['depends', 'application', 'cosine', 'around', 'debate', 'similarities', 'upon', 'scale', 'students', 'angle', 'specified']\n",
      "140      ['event', 'diagnosis', 'correct', 'proposed', 'formal']\n",
      "141      ['instance', 'categorization', 'support', 'representation', 'thus', 'produced']\n",
      "142      ['keywords', 'single', 'longer', 'phrases']\n",
      "143      ['like', 'site', 'patent', 'ranking', 'actual', 'algorithms', 'factors', 'meaning', 'webpage', 'denotes', 'toolbar', 'articles', 'influence', 'modern', 'through']\n",
      "144      ['000222005', '000641574', '000782035']\n",
      "145      ['000714138', '000679629', '00102065', '00133258', '00139828']\n",
      "146      ['000837489', '00024084', '000253858', '000306806']\n",
      "147      ['00105573', '000218071', '000346468', '000722866', '00118384']\n",
      "148      ['00127245', '00027091', '000641772', '00127034']\n",
      "149      ['ancestor', 'exposed', 'those', 'adding']\n",
      "150      ['false', 'match', 'applications', 'negative', 'positive', 'might', 'won', 'associated', 'recursive', 'matches', 'cases', 'frequentists', 'substrings']\n",
      "151      ['plan', 'action']\n",
      "153      ['structure', 'kind']\n",
      "154      ['weighting', 'path', 'higher', 'characteristics', 'others', 'your', 'subproblem', 'goal', 'users', 'visits', 'relevance', 'relevant', 'among', 'outbound', 'website', 'affect', 'us']\n",
      "155      ['000104496', '00134634', '0015882', '0015892']\n",
      "156      ['000112297', '000510361', '000822361', '000851915', '000970411', '00112897']\n",
      "157      ['000118776', '000257429', '0002997', '000379887', '000873691', '00144738']\n",
      "158      ['000121415', '000289281', '000741238', '00121285', '00127489']\n",
      "159      ['000121931', '000943677', '00132224', '00163446']\n",
      "160      ['000128402', '000935097', '000997195', '00127242']\n",
      "161      ['000146166', '000228113', '000784815', '000914112']\n",
      "162      ['000149611', '000254484', '000342545', '000481291', '000717298', '000918943', '00103384', '00107373', '00120731', '00125502', '00134052', '00135353', '0015764', '00159165', '28277e', '74935e', '77299e', '98124e', '07', '000295394']\n",
      "163      ['000154344', '000597976', '0014425', '0015844']\n",
      "164      ['000154858', '000658096', '000740185', '00143611']\n",
      "165      ['000173975', '000828879', '00134573', '00138859']\n",
      "166      ['000176249', '000923061', '0011547', '00150916']\n",
      "167      ['000204662', '000233989', '00132652', '00162791']\n",
      "168      ['000208043', '00028446', '000388528', '000486314', '000972417', '00130591', '00154554', '63164e']\n",
      "169      ['000209893', '000658771', '00146592']\n",
      "170      ['000213784', '00044146', '00130641', '00132182']\n",
      "171      ['000218611', '000627347', '00147324']\n",
      "172      ['000228605', '000247751', '000507011', '000510669']\n",
      "173      ['000230336', '000309389', '0010244']\n",
      "174      ['000233216', '000302909', '000672392', '00121509']\n",
      "175      ['000237438', '000393385', '000533912', '000699741']\n",
      "176      ['000251048', '000419433', '0009751', '00158112']\n",
      "177      ['000253155', '00060477', '00143099']\n",
      "178      ['000253817', '000354644', '000479967', '000661417', '00160269']\n",
      "179      ['000281587', '000392922', '000689932', '000692842', '00116315', '00124763', '00153979']\n",
      "180      ['00028579', '000606723', '00127091', '00145284']\n",
      "181      ['000288034', '00122644', '00145348', '7908e', '79304e']\n",
      "182      ['000288376', '00109647', '00159964', '00165102']\n",
      "183      ['00029439', '000388454', '00110349']\n",
      "184      ['000339263', '00062685', '000767073', '000967188']\n",
      "185      ['000344981', '000664931', '00067521', '00104599']\n",
      "186      ['000349485', '000723977', '000797545', '00153604']\n",
      "187      ['000373652', '000392118', '00160403']\n",
      "188      ['000385199', '00116003', '00125393']\n",
      "189      ['00038978', '000468307', '000606068', '00147916']\n",
      "190      ['000404928', '000681381', '00111769', '00142694', '00156054']\n",
      "191      ['000407494', '000654478', '00106799']\n",
      "192      ['000429939', '00075486', '000993489', '00104788', '00124217']\n",
      "193      ['000434826', '000997342', '00147126']\n",
      "194      ['000437583', '00106998', '00112648', '001343']\n",
      "195      ['000438051', '000464908', '00121033', '69545e']\n",
      "196      ['000454297', '000615239', '00123691']\n",
      "197      ['000466403', '00118882', '00148223']\n",
      "198      ['00046674', '000766635', '00109457', '0122e']\n",
      "199      ['000509222', '00117286', '0016061', '00162254']\n",
      "200      ['000515494', '000920251', '000978992']\n",
      "201      ['000521099', '00101139', '00113677', '00120606', '00129875']\n",
      "202      ['00053947', '000894328', '00149773', '00153857']\n",
      "203      ['000543594', '000599943', '000788925', '000861262', '00159783']\n",
      "204      ['000555889', '000568793', '000965391']\n",
      "205      ['000558035', '00070198', '000721091', '00105603', '00122734']\n",
      "206      ['000566388', '000727566', '00095051', '00143176']\n",
      "207      ['000592722', '00076664', '00078398', '00154063']\n",
      "208      ['000624961', '000769519', '000948308', '00149124', '00150931']\n",
      "209      ['000679533', '00126601']\n",
      "210      ['000798503', '00104602', '00148124']\n",
      "211      ['000818812', '000908618', '1804e']\n",
      "212      ['000820195', '000955627', '0669e']\n",
      "213      ['000863904', '00133235', '0016119']\n",
      "214      ['000884418', '00146811']\n",
      "215      ['000927189', '00110873', '00123337', '00159539']\n",
      "216      ['00100001', '00148554', '00160493']\n",
      "217      ['00112502', '00116851', '00122709', '00166282']\n",
      "218      ['00118601', '00122127', '00158507', '00162762']\n",
      "219      ['00137624', '00141866', '00142988', '00159927']\n",
      "221      ['general', 'referred', 'seen']\n",
      "222      ['less', 'much', 'account', 'cognitive', 'economy']\n",
      "223      ['world', 'wide']\n",
      "224      ['assigned', 'should']\n",
      "225      ['assigns', 'engine', 'simple', 'until', 'case', 'clicking', 'person', 'randomly', 'provided']\n",
      "226      ['keyword', 'concept', 'graph', 'valid', 'equal', 'originally', 'invented']\n",
      "227      ['trousers', 'central', 'plays', 'role', 'debates', 'greater']\n",
      "229      ['child', 'little', 'over', 'parent']\n",
      "230      ['either', 'accomplished', 'dividing', 'overriding', 'replacing']\n",
      "233      ['tf', 'idf']\n",
      "234      ['according', 'assign']\n",
      "235      ['bottom', 'hijubx', 'university', 'comes', 'shortest', 'stanford', 'had', 'naturally', 'references', 'rh', 'ru', 'qu', 'refined', 'rz', 'sn', 'weight']\n",
      "236      ['dimension', 'separate', 'corresponds']\n",
      "238      ['relative', 'definition', 'dimensionality', 'advantage', 'complexity', 'measuring', 'reducing', 'foundations', 'inbound', 'purpose', 'total', 'distinct']\n",
      "239      ['representing', 'text']\n",
      "240      ['take', 'prob', '1953']\n",
      "241      ['weights', 'developed']\n",
      "243      ['indexing', 'rankings', 'relevancy', 'filtering', 'smart']\n",
      "244      ['step', 'three']\n",
      "245      ['provides', 'basic', 'constant', 'appears', 'basically', 'field', 'remembered', 'solvable']\n",
      "246      ['exhibition', 'observations', 'powerful', 'computation', 'design', 'technique', 'modification']\n",
      "247      ['control', 'learning', 'shared', 'controlled', 'human']\n",
      "248      ['schedule', 'finalized']\n",
      "249      ['thomas', 'law', 'rev']\n",
      "250      ['polymorphism', 'view', 'dual']\n",
      "251      ['yo', 'shares', 'vanishing']\n"
     ]
    }
   ],
   "source": [
    "clust_dict = {}\n",
    "for clus in clusters:\n",
    "    if len(clus) == 1:\n",
    "        if 0 not in clust_dict:\n",
    "            clust_dict[0] = clus\n",
    "        else:\n",
    "            clust_dict[0] += clus\n",
    "    else:\n",
    "        clust_dict[clusters.index(clus)] = clus\n",
    "for key, value in clust_dict.items():\n",
    "    print(key, '    ', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, nltk, re\n",
    "\n",
    "# Open a file\n",
    "path = \"/Users/varunnandu/Desktop/plagiarism-detection-nlp/DATA-NLP\"\n",
    "dirs = os.listdir( path )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_mapping = dict() # Mapping to question dictionary\n",
    " # Mapping to student as key and answer as value\n",
    "\n",
    "for file in dirs:\n",
    "    if file == '.DS_Store':\n",
    "            continue\n",
    "    path_to_file='/Users/varunnandu/Desktop/plagiarism-detection-nlp/DATA-NLP/'\n",
    "    file_name = file\n",
    "    split_name = file.split('_')\n",
    "    student_name = split_name[0]\n",
    "    question_number = split_name[1].split('.')[0]\n",
    "    if question_number not in question_mapping:\n",
    "        question_mapping[question_number] = {}\n",
    "    path_to_file += file_name\n",
    "    with open(path_to_file, 'r', errors = 'ignore') as f:\n",
    "        mylist = f.read()\n",
    "        sent_tokenize_list = nltk.sent_tokenize(mylist)\n",
    "        temp = []\n",
    "        for i in sent_tokenize_list:\n",
    "            sent = i.lower()\n",
    "            sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "            temp.append(sent)\n",
    "        question_mapping[question_number][student_name] = temp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dict = {}\n",
    "for key, value in question_mapping.items():\n",
    "    vector_dict[key] = {}\n",
    "    for student, answer in value.items():\n",
    "        vector_dict[key][student] = []\n",
    "        for sent in answer:\n",
    "            words = sent.split(' ')\n",
    "            temp_list = []\n",
    "            for w in words:\n",
    "                for cluster_key, cluster_value in clust_dict.items():\n",
    "                    if w in clust_dict[cluster_key]:\n",
    "                        temp_list.append( cluster_key)\n",
    "            vector_dict[key][student].append(temp_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_name = {}\n",
    "rep = 0\n",
    "for key, value in vector_dict.items():\n",
    "    for student, vector in value.items():\n",
    "        for v in vector:\n",
    "            vector_name[tuple(v)] = rep\n",
    "            rep+=1\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taskc      {'g0pC': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'g0pE': [9], 'g1pA': [10, 11, 12, 76, 14, 15, 16, 17, 18, 19, 20], 'g3pB': [21, 22, 23, 24, 25, 26, 27, 28, 29], 'g0pB': [30, 74, 153, 108, 137, 78, 133, 134, 82, 39, 40, 138, 42], 'g2pA': [43, 153, 45, 46, 47, 48, 49, 50, 51, 133, 53, 82, 55, 56, 57, 86, 909, 60], 'g0pD': [61, 62, 63, 64, 65, 66, 67, 68, 137, 70, 71], 'orig': [72, 153, 74, 155, 76, 137, 78, 79, 133, 134, 82, 83, 84, 85, 86, 909, 118], 'g3pC': [89, 90, 91, 92, 93, 94, 95], 'g2pB': [96, 97, 98, 99, 100, 101, 102, 103, 104, 105], 'g0pA': [106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118], 'g4pC': [119, 120, 909, 122, 909, 124, 750, 909, 127, 909, 129, 909, 131, 132], 'g4pE': [133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143], 'g2pC': [144, 145, 146, 147, 148, 149, 150, 151], 'g4pB': [152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163], 'g1pD': [164, 165, 166, 167], 'g4pD': [168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183], 'g2pE': [184, 185, 186, 187, 188], 'g3pA': [189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201], 'g1pB': [202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217]}\n",
      "taskd      {'g0pE': [218, 219, 220], 'g3pB': [221, 222, 223, 224, 225, 226], 'g1pA': [227, 228, 229, 372, 231, 232, 233], 'g0pC': [367, 235, 236, 237, 370, 239, 240, 241], 'g0pD': [242, 243], 'g3pC': [244, 245, 246, 247, 248], 'orig': [367, 392, 333, 369, 253, 371, 372, 373, 374, 375, 316, 386, 318, 386, 320, 378], 'g2pA': [367, 392, 333, 369, 370, 371, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284], 'g0pB': [360, 286, 287, 288, 386, 290, 291, 292, 293, 294, 295, 296], 'g4pE': [297, 298, 909, 300, 301, 302, 303, 304, 305], 'g4pC': [391, 307, 308, 369, 370, 371, 373, 313, 314, 315, 316, 386, 318, 386, 320, 378], 'g0pA': [322, 323, 324, 325, 326, 327, 328, 329, 330], 'g2pB': [331, 392, 333, 334, 335, 336, 337, 338, 373, 340, 341, 342, 343], 'g2pE': [344, 345, 346, 347, 348, 354], 'g4pD': [350, 351, 352, 353, 354, 355, 356, 357, 358, 359], 'g1pB': [360, 361, 362, 363, 364, 365, 366], 'g3pA': [367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378], 'g4pB': [379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390], 'g2pC': [391, 392, 393, 394, 395, 396, 397, 398, 399], 'g1pD': [400, 401, 402, 403, 404, 405, 406]}\n",
      "taske      {'g0pE': [407, 528, 622, 602, 603, 623], 'g1pA': [413, 414, 415, 416, 417, 418, 419, 420, 421, 422], 'g3pB': [423, 424, 622, 426, 603, 428, 632, 624], 'g0pC': [431, 432, 433, 599, 559, 436, 437, 438, 439, 440], 'g0pD': [441, 442, 443, 444, 445], 'orig': [446, 528, 598, 599, 559, 600, 622, 602, 603, 623, 624, 457, 625, 626, 909, 628, 909, 630, 631, 909, 466, 467, 632, 633, 634, 635, 636, 637, 474, 638, 639], 'g3pC': [477, 478, 479, 480, 481, 482], 'g0pB': [483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 909, 494, 909, 496, 497], 'g2pA': [498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513], 'g4pE': [514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 909, 525, 526], 'g2pB': [621, 528, 529, 530, 602, 603, 623, 624, 535, 536, 628, 630, 631], 'g0pA': [540, 541, 542, 543, 544, 909, 546, 909, 548, 909, 550, 551, 552, 553, 554], 'g4pC': [555, 556, 557, 599, 559, 560, 561, 562, 563, 624, 565, 566, 567], 'g4pD': [568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 634, 579], 'g2pE': [580, 581, 582, 583, 584, 585, 586, 587], 'g3pA': [588, 589, 590, 591, 592, 593, 594, 595, 596], 'g1pB': [597, 598, 599, 600, 622, 602, 603, 623, 605, 606, 607, 608, 609, 610], 'g2pC': [611, 612, 613, 614, 615, 616, 617, 618, 619, 620], 'g4pB': [621, 622, 623, 624, 625, 626, 909, 628, 909, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640], 'g1pD': [641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651]}\n",
      "taskb      {'g0pC': [652, 653, 654, 655, 656, 657, 658, 659, 660], 'g0pE': [802, 753, 663], 'g3pB': [664, 665, 666, 667, 668, 669, 670, 671, 672, 673], 'g1pA': [674, 675, 676, 677, 678, 679, 680, 681, 682], 'g2pA': [683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694], 'g0pB': [695, 696, 697, 698, 699, 700, 701], 'g0pD': [702, 703, 704, 705], 'g3pC': [706, 707, 708, 709, 710, 711, 712], 'orig': [802, 753, 775, 776, 777, 778, 719, 720, 721, 722, 723, 779, 780, 781, 727, 728, 803, 804, 731, 756, 757, 734, 782, 783, 737], 'g4pC': [738, 739, 740, 741, 742, 743, 744], 'g0pA': [802, 746, 747, 750, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759], 'g2pB': [760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770], 'g4pE': [771, 772, 773, 776, 775, 776, 777, 778, 779, 780, 781, 782, 783], 'g4pB': [784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794], 'g2pC': [795, 796, 797, 798, 799, 800, 801], 'g1pD': [802, 803, 804, 820, 806, 822, 808, 824, 826, 827, 812, 813], 'g2pE': [814, 815, 816, 817, 818, 819], 'g4pD': [820, 909, 822, 823, 824, 909, 826, 827, 828, 829, 830, 831, 832, 833], 'g1pB': [834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844], 'g3pA': [845, 846, 847, 848, 849, 850, 851, 852, 853]}\n",
      "taska      {'g0pC': [854, 855, 856, 857, 858, 859, 860, 861, 862, 863], 'g3pB': [864, 865, 866, 867, 868, 869, 870, 871, 872, 873], 'g1pA': [874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885], 'g0pE': [954, 933, 888, 1030, 936, 937, 959, 960, 961, 962, 1031, 1032, 1033], 'g2pA': [899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913], 'g0pB': [914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925], 'g3pC': [954, 959, 960, 961, 930, 1033], 'orig': [954, 933, 1029, 1030, 936, 937, 959, 960, 961, 962, 1031, 1032, 1033, 966], 'g0pD': [946, 947, 948, 949, 950, 961, 952, 1031], 'g4pC': [954, 955, 956, 1029, 1030, 959, 960, 961, 962, 1031, 1032, 1033, 966], 'g0pA': [967, 968, 969, 970, 971, 972, 973, 974], 'g2pB': [975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987], 'g4pE': [988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998], 'g1pD': [999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009], 'g4pB': [1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027], 'g2pC': [1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037], 'g1pB': [1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048], 'g3pA': [1049, 1050, 1051, 1052, 1053, 1054, 1055], 'g2pE': [1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068], 'g4pD': [1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079]}\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "pro_dict = {}\n",
    "for key, value in vector_dict.items():\n",
    "    pro_dict[key] = {}\n",
    "    for student, answer in value.items():\n",
    "        pro_dict[key][student] = []\n",
    "        for v in answer:\n",
    "            ans = tuple(v)\n",
    "            pro_dict[key][student].append(vector_name[ans])\n",
    "\n",
    "for key, value in pro_dict.items():\n",
    "    print(key, '    ', value)\n",
    "    \n",
    "print(len(pro_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyfpgrowth\n",
    "transactions = []\n",
    "inp = \"taskd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[218, 219, 220], [221, 222, 223, 224, 225, 226], [227, 228, 229, 372, 231, 232, 233], [367, 235, 236, 237, 370, 239, 240, 241], [242, 243], [244, 245, 246, 247, 248], [367, 392, 333, 369, 253, 371, 372, 373, 374, 375, 316, 386, 318, 386, 320, 378], [367, 392, 333, 369, 370, 371, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284], [360, 286, 287, 288, 386, 290, 291, 292, 293, 294, 295, 296], [297, 298, 909, 300, 301, 302, 303, 304, 305], [391, 307, 308, 369, 370, 371, 373, 313, 314, 315, 316, 386, 318, 386, 320, 378], [322, 323, 324, 325, 326, 327, 328, 329, 330], [331, 392, 333, 334, 335, 336, 337, 338, 373, 340, 341, 342, 343], [344, 345, 346, 347, 348, 354], [350, 351, 352, 353, 354, 355, 356, 357, 358, 359], [360, 361, 362, 363, 364, 365, 366], [367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378], [379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390], [391, 392, 393, 394, 395, 396, 397, 398, 399], [400, 401, 402, 403, 404, 405, 406]]\n"
     ]
    }
   ],
   "source": [
    "for k, v in pro_dict.items():\n",
    "    if k == inp:\n",
    "        for name, vec in v.items():\n",
    "            transactions.append(vec)\n",
    "print(transactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[373, 378, 386], [371, 373, 378], [371, 373, 378, 386], [371, 378, 386], [369, 371, 378], [369, 371, 378, 386], [369, 371, 373, 378], [369, 371, 373, 378, 386], [369, 373, 378, 386], [369, 378, 386], [367, 369, 371], [369, 370, 371], [369, 371, 386], [371, 373, 386], [369, 371, 373], [369, 371, 373, 386], [369, 373, 386]]\n"
     ]
    }
   ],
   "source": [
    "patterns = pyfpgrowth.find_frequent_patterns(transactions, 3)\n",
    "frequent_list = []\n",
    "for p in patterns:\n",
    "    if len(p) < 3:\n",
    "        continue\n",
    "    else:\n",
    "        frequent_list.append(list(p))\n",
    "print(frequent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_sub(sub, lst):\n",
    "    ln = len(sub)\n",
    "    for i in range(len(lst) - ln + 1):\n",
    "        if all(sub[j] == lst[i+j] for j in range(ln)):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'taskd': ['g2pA', 'g4pC', 'g3pA']}\n",
      "Question taskd was copied from g2pA\n",
      "Question taskd was copied from g4pC\n",
      "Question taskd was copied from g3pA\n"
     ]
    }
   ],
   "source": [
    "output_dict= {}\n",
    "for lst in frequent_list:\n",
    "    for k, v in pro_dict.items():\n",
    "        for name, vec in v.items():\n",
    "            if is_sub(lst,vec):\n",
    "                if k not in output_dict:\n",
    "                    output_dict[k] = [name]\n",
    "                else:\n",
    "                    output_dict[k].append(name)\n",
    "\n",
    "print(output_dict)\n",
    "for k, v in output_dict.items():\n",
    "    for item in v:\n",
    "        print('Question ' + k + ' was copied from ' + item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "xl = pd.ExcelFile(\"corpus_final.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = xl.parse(\"File list\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df[['File','Category']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is :  1.0\n",
      "Recall is :  0.125\n",
      "Accuracy is :  0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "fp = 0\n",
    "for key, value in output_dict.items():\n",
    "    for v in value:\n",
    "        ans = v+key+'.txt'\n",
    "        head = key\n",
    "        for index, row in c.iterrows():\n",
    "            if row['File'].split('_')[1].split('.')[0] == head:\n",
    "                if row['File'].split('_')[0] == v:\n",
    "                    if row['Category'] == 'cut' or row['Category'] == 'heavy':\n",
    "                        tp+=1\n",
    "                    else:\n",
    "                        fp+=1\n",
    "                else:\n",
    "                    if row['Category'] == 'cut' or row['Category'] == 'heavy':\n",
    "                        fn +=1\n",
    "                    else:\n",
    "                        tn+=1\n",
    "\n",
    "precision = float(tp/(tp+fp))                        \n",
    "print('Precision is : ', precision)\n",
    "recall = float(tp/(tp+fn))\n",
    "print('Recall is : ', recall)\n",
    "print('Accuracy is : ', (tp+tn)/(tp+fn+tn+fp))\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
